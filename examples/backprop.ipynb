{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Overview of Backpropagation**\n",
    "Backpropagation is the algorithm used to train neural networks by minimizing the error between the predicted output and the actual target. It involves:\n",
    "- **Forward pass:** Compute the output of the network.\n",
    "- **Backward pass:** Compute gradients of the loss with respect to the network's parameters using the chain rule.\n",
    "- **Parameter update:** Adjust weights using gradient descent.\n",
    "\n",
    "### 2. **Forward Pass**\n",
    "For a single layer, the output is:\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "$$\n",
    "a^{(l)} = \\sigma(z^{(l)})\n",
    "$$\n",
    "Here:\n",
    "- $ W^{(l)} $: Weights of layer $ l $\n",
    "- $ b^{(l)} $: Bias of layer $ l $\n",
    "- $ a^{(l-1)} $: Activations from the previous layer\n",
    "- $ \\sigma $: Activation function\n",
    "\n",
    "The output of the network is:\n",
    "$$\n",
    "\\hat{y} = a^{(L)}\n",
    "$$\n",
    "\n",
    "### 3. **Loss Function**\n",
    "Define the loss function $ \\mathcal{L} $. For example, for mean squared error:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} \\sum_i (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### 4. **Backward Pass**\n",
    "#### Gradient Computation\n",
    "The goal is to compute gradients of $ \\mathcal{L} $ with respect to all weights and biases.\n",
    "\n",
    "**Step 1: Compute error at the output layer**\n",
    "$$\n",
    "\\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}} = (a^{(L)} - y) \\odot \\sigma'(z^{(L)})\n",
    "$$\n",
    "\n",
    "**Step 2: Backpropagate error to hidden layers**\n",
    "\n",
    "For each layer $ l $ (going backward):\n",
    "$$\n",
    "\\delta^{(l)} = (W^{(l+1)T} \\delta^{(l+1)}) \\odot \\sigma'(z^{(l)})\n",
    "$$\n",
    "\n",
    "**Step 3: Gradients for weights and biases**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\delta^{(l)} {a^{(l-1)}}^T\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} = \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ \\delta^{(l)} $: Error term at layer $ l $\n",
    "- $ \\sigma'(z^{(l)}) $: Derivative of the activation function\n",
    "\n",
    "### 5. **Parameter Update**\n",
    "Using gradient descent, update parameters:\n",
    "$$\n",
    "W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
    "$$\n",
    "$$\n",
    "b^{(l)} \\leftarrow b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
    "$$\n",
    "Here, $\\eta $ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us walk through an example using numerical values, visualizations, and a Python implementation with NumPy to clarify the backpropagation process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Neural Network Setup**\n",
    "Weâ€™ll use a small neural network with:\n",
    "- 1 input layer (2 neurons)\n",
    "- 1 hidden layer (2 neurons)\n",
    "- 1 output layer (1 neuron)\n",
    "\n",
    "#### Initial weights and biases:\n",
    "- $W^{(1)} = \\begin{bmatrix} 0.15 & 0.25 \\\\ 0.20 & 0.30 \\end{bmatrix}, b^{(1)} = \\begin{bmatrix} 0.35 \\\\ 0.35 \\end{bmatrix} $\n",
    "- $W^{(2)} = \\begin{bmatrix} 0.40 & 0.50 \\end{bmatrix}, b^{(2)} = 0.60 $\n",
    "\n",
    "#### Input:\n",
    "$$\n",
    "x = \\begin{bmatrix} 0.05 \\\\ 0.10 \\end{bmatrix}, \\quad y = 0.01\n",
    "$$\n",
    "\n",
    "### **Forward Pass (Numerical Example)**\n",
    "The steps involve computing activations layer by layer.\n",
    "\n",
    "#### 1. **Hidden Layer**:\n",
    "$$\n",
    "z^{(1)} = W^{(1)}x + b^{(1)} = \\begin{bmatrix} 0.15 & 0.25 \\\\ 0.20 & 0.30 \\end{bmatrix} \\begin{bmatrix} 0.05 \\\\ 0.10 \\end{bmatrix} + \\begin{bmatrix} 0.35 \\\\ 0.35 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "z^{(1)} = \\begin{bmatrix} 0.3775 \\\\ 0.3925 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Apply the sigmoid activation function:\n",
    "$$\n",
    "a^{(1)} = \\sigma(z^{(1)}) = \\frac{1}{1 + e^{-z^{(1)}}}\n",
    "$$\n",
    "$$\n",
    "a^{(1)} = \\begin{bmatrix} 0.59327 \\\\ 0.59688 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. **Output Layer**:\n",
    "$$\n",
    "z^{(2)} = W^{(2)}a^{(1)} + b^{(2)} = \\begin{bmatrix} 0.40 & 0.50 \\end{bmatrix} \\begin{bmatrix} 0.59327 \\\\ 0.59688 \\end{bmatrix} + 0.60\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = 1.10591\n",
    "$$\n",
    "\n",
    "Apply the sigmoid activation:\n",
    "$$\n",
    "\\hat{y} = \\sigma(z^{(2)}) = \\frac{1}{1 + e^{-z^{(2)}}}\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = 0.75136\n",
    "$$\n",
    "\n",
    "#### Loss:\n",
    "Using Mean Squared Error:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} (y - \\hat{y})^2 = \\frac{1}{2} (0.01 - 0.75136)^2 = 0.27407\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Backward Pass (Numerical Example)**\n",
    "\n",
    "#### 1. **Output Layer Gradients**:\n",
    "Error at the output layer:\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}} = (\\hat{y} - y) \\cdot \\sigma'(z^{(2)})\n",
    "$$\n",
    "$$\n",
    "\\sigma'(z^{(2)}) = \\hat{y}(1 - \\hat{y}) = 0.75136 \\cdot (1 - 0.75136) = 0.186815\n",
    "$$\n",
    "$$\n",
    "\\delta^{(2)} = (0.75136 - 0.01) \\cdot 0.186815 = 0.13849\n",
    "$$\n",
    "\n",
    "Gradients for $W^{(2)} $ and $b^{(2)} $:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\delta^{(2)} \\cdot a^{(1)} = \\begin{bmatrix} 0.08220 \\\\ 0.08270 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} = \\delta^{(2)} = 0.13849\n",
    "$$\n",
    "\n",
    "#### 2. **Hidden Layer Gradients**:\n",
    "Error at the hidden layer:\n",
    "$$\n",
    "\\delta^{(1)} = ({W^{(2)}}^T \\delta^{(2)}) \\cdot \\sigma'(z^{(1)})\n",
    "$$\n",
    "$$\n",
    "\\sigma'(z^{(1)}) = a^{(1)}(1 - a^{(1)})\n",
    "$$\n",
    "$$\n",
    "\\delta^{(1)} = \\begin{bmatrix} 0.05 \\\\ 0.06 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Gradients for $W^{(1)} $ and $b^{(1)} $:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\delta^{(1)} \\cdot x^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Implementation**\n",
    "\n",
    "Let us implement this in Python using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for W2: [[0.08169586 0.08194416]]\n",
      "Gradients for b2: [[0.13742501]]\n",
      "Gradients for W1: [[0.00066259 0.00132519]\n",
      " [0.00082706 0.00165411]]\n",
      "Gradients for b1: [[0.01325186]\n",
      " [0.01654114]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Inputs and target\n",
    "x = np.array([[0.05], [0.10]])\n",
    "y = 0.01\n",
    "\n",
    "# Initial weights and biases\n",
    "W1 = np.array([[0.15, 0.25], [0.20, 0.30]])\n",
    "b1 = np.array([[0.35], [0.35]])\n",
    "W2 = np.array([[0.40, 0.50]])\n",
    "b2 = np.array([[0.60]])\n",
    "\n",
    "# Forward pass\n",
    "z1 = np.dot(W1, x) + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "a2 = sigmoid(z2)\n",
    "loss = 0.5 * (y - a2)**2\n",
    "\n",
    "# Backward pass\n",
    "delta2 = (a2 - y) * sigmoid_derivative(z2)\n",
    "grad_W2 = np.dot(delta2, a1.T)\n",
    "grad_b2 = delta2\n",
    "\n",
    "delta1 = np.dot(W2.T, delta2) * sigmoid_derivative(z1)\n",
    "grad_W1 = np.dot(delta1, x.T)\n",
    "grad_b1 = delta1\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients for W2:\", grad_W2)\n",
    "print(\"Gradients for b2:\", grad_b2)\n",
    "print(\"Gradients for W1:\", grad_W1)\n",
    "print(\"Gradients for b1:\", grad_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Sigmoid Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
      "Tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "Tanh Derivative: [0.07065082 0.41997434 1.         0.41997434 0.07065082]\n",
      "ReLU: [0. 0. 0. 1. 2.]\n",
      "ReLU Derivative: [0 0 0 1 1]\n",
      "Leaky ReLU: [-0.02 -0.01  0.    1.    2.  ]\n",
      "Leaky ReLU Derivative: [0.01 0.01 0.01 1.   1.  ]\n",
      "Softmax: [0.65900114 0.24243297 0.09856589]\n",
      "Softmax Derivative: [[ 0.22471864 -0.1597636  -0.06495503]\n",
      " [-0.1597636   0.18365923 -0.02389562]\n",
      " [-0.06495503 -0.02389562  0.08885066]]\n"
     ]
    }
   ],
   "source": [
    "# Mathematical Formulas and Python Implementation of Activation Functions\n",
    "\n",
    "# --- Mathematical Formulas ---\n",
    "# Sigmoid:          f(x) = 1 / (1 + e^(-x))\n",
    "# Sigmoid Derivative: f'(x) = f(x) * (1 - f(x))\n",
    "\n",
    "# Tanh:             f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "# Tanh Derivative:  f'(x) = 1 - f(x)^2\n",
    "\n",
    "# ReLU:             f(x) = max(0, x)\n",
    "# ReLU Derivative:  f'(x) = 1 if x > 0 else 0\n",
    "\n",
    "# Leaky ReLU:       f(x) = x if x > 0 else alpha * x (alpha is a small constant, e.g., 0.01)\n",
    "# Leaky ReLU Derivative: f'(x) = 1 if x > 0 else alpha\n",
    "\n",
    "# Softmax:          f(x)_i = e^(x_i) / sum(e^(x_j)) for all j\n",
    "# Softmax Derivative: f'(x)_i = f(x)_i * (1 - f(x)_i) for the same class, \n",
    "#                     and -f(x)_i * f(x)_j for different classes\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "# Tanh Activation Function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# ReLU Activation Function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Leaky ReLU Activation Function\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "# Softmax Activation Function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Stability improvement by subtracting max(x)\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "def softmax_derivative(softmax_output):\n",
    "    s = softmax_output.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "# Examples for each activation function\n",
    "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"Sigmoid Derivative:\", sigmoid_derivative(x))\n",
    "\n",
    "print(\"Tanh:\", tanh(x))\n",
    "print(\"Tanh Derivative:\", tanh_derivative(x))\n",
    "\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"ReLU Derivative:\", relu_derivative(x))\n",
    "\n",
    "print(\"Leaky ReLU:\", leaky_relu(x))\n",
    "print(\"Leaky ReLU Derivative:\", leaky_relu_derivative(x))\n",
    "\n",
    "x_for_softmax = np.array([2.0, 1.0, 0.1])\n",
    "softmax_output = softmax(x_for_softmax)\n",
    "print(\"Softmax:\", softmax_output)\n",
    "print(\"Softmax Derivative:\", softmax_derivative(softmax_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
